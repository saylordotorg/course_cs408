---
layout: default
title: "CS408: Advanced Artificial Intelligence"
course_description: "A detailed examination of the concepts and methods of artificial intelligence. Topics include heuristic search procedures for general graphs, game playing strategies, resolution and rule based deduction systems, knowledge representation, and reasoning with uncertainty."
next: ../Unit05
previous: ../Unit03
---
**Unit 4: Learning** <span id="4"></span> 
*This unit presents an artificial neural network (NN) as the most
important learning tool for machine learning.  Machine learning research
tries to automatically extract information from data through
computational and statistical methods.  Machine learning is closely
related to not only data mining and statistics, but also theoretical
computer science.  NN is a computational model based on biological
neural networks.  It consists of an interconnected group of artificial
neurons and processes.  Practically, neural networks are non-linear
statistical data modeling tools used to model complex relationships
between inputs and outputs.  After being successfully trained, NNs are
able to perform classification, estimation, prediction, or simulation
with new data.  The second part of this unit reviews the Gaussian and
Bayesian processes used in machine learning.*

**Unit 4 Time Advisory**  
This unit should take you 35 hours to complete.  
  
 ☐    Subunit 4.1: 9 hours

☐    Subunit 4.2: 8 hours

☐    Subunit 4.3: 8 hours

☐    Subunit 4.4: 10 hours

**Unit4 Learning Outcomes**  
Upon successful completion of this unit, students will be able to:

-   Define and list learning algorithm types.
-   Define and list major neural network types, including the Perceptron
    network, the Feedforward Neural Network, and the Hopfield Network.
-   Define and compare/contrast Gaussian and Bayesian processes.
-   Design a simple decision-making software agent.

**4.1 Machine Learning** <span id="4.1"></span> 
-   **Reading: Wikipedia’s “Machine Learning”**
    Link: Wikipedia’s “[Machine
    Learning](http://www.saylor.org/site/wp-content/uploads/2012/06/Wikipedia-Machine-Learning.pdf)”
    (PDF)  
        
     Instructions: Read this web page for an overview of machine
    learning.  Be sure you understand the differences between the
    learning methods, which you can read about beneath the 'Algorithm
    types' section.  This reading covers subsections 4.1.1-4.1.5.  
        
     Terms of Use: The article above is released under a [Creative
    Commons Attribution-Share Alike License
    3.0](http://creativecommons.org/licenses/by-sa/3.0/) (HTML).  You
    can find the Wikipedia source article
    [here](http://en.wikipedia.org/wiki/Machine_learning) (HTML).

**4.1.1 Supervised Learning** <span id="4.1.1"></span> 
**4.1.2 Unsupervised Learning** <span id="4.1.2"></span> 
**4.1.3 Reinforcement Learning** <span id="4.1.3"></span> 
-   **Lecture: videolectures.net: John Lloyd’s “Intelligent Agents: Part
    2”**
    Link: videolectures.net: John Lloyd’s “[Intelligent Agents: Part
    2](http://videolectures.net/ssll09_lloyd_inta/)” (Adobe Flash and
    Windows Media Player)  
        
     Instructions: Watch the second part of the video by John Lloyd and
    pay attention to the AIMA learning agent.  Please compare Lloyd’s
    explanation of reinforced learning with the definition provided on
    the World Lingo site. This video is 50 minutes long.  You can also
    download the PowerPoint slides in a PDF format by clicking on the
    link under “See Also.”  
        
     About the link: John Lloyd is a professor at Australian National
    University who shares lectures on videolectures.net.  
        
     Terms of Use: The article above is released under a [Creative
    Commons Attribution-NonCommercial- NoDerivatives License
    3.0](http://creativecommons.org/licenses/by-nc-nd/3.0/)(HTML).  It
    is attributed to (John Lloyd).

**4.1.4 Transduction** <span id="4.1.4"></span> 
**4.1.5 Multi-task Learning** <span id="4.1.5"></span> 
**4.1.6 Machine Learning, Probability, and Graphical Models** <span
id="4.1.6"></span> 
-   **Lecture: videolectures.net: Sam Roweis’s “Machine Learning,
    Probability, and Graphical Models: Part 1”**
    Link: videolectures.net: Sam Roweis’s “[Machine Learning,
    Probability, and Graphical Models: Part
    1](http://videolectures.net/mlss06tw_roweis_mlpgm/)” (Adobe Flash
    and Windows Media Player)  
        
     Instructions: Watch the first part of the video by Sam Roweisto
    review the applications of probabilistic learning, the concept of
    representation, and examples of training and graphical models.  You
    may wish to work through the slides available on the left-hand side
    of this web-page as you listen to Professor Roweis’ lecture.  This
    video is just over 1 hour long. You can also download the PowerPoint
    slides in a PDF format by clicking on the link under “See Also.”  
        
     About the link: Sam Roweis is an Associate Professor in the
    Department of Computer Science at the University of Toronto.  His
    research interests include machine learning, data mining, and
    statistical signal processing.  
        
     Terms of Use: The article above is released under a [Creative
    Commons Attribution-NonCommercial- NoDerivatives License
    3.0](http://creativecommons.org/licenses/by-nc-nd/3.0/)(HTML).  It
    is attributed to (John Lloyd).

**4.2 Neural Network** <span id="4.2"></span> 
**4.2.1 Introduction to Neural Networks** <span id="4.2.1"></span> 
-   **Reading: Wolfram Mathematica’s “Introduction to Neural Networks”**
    Link: Wolfram Mathematica’s “[Introduction to Neural
    Networks](http://reference.wolfram.com/applications/neuralnetworks/NeuralNetworkTheory/2.1.0.html)”
    (HTML)  
        
     Instructions: Read the general 2.1 section to learn about general
    neural networks and how they are mathematically defined.   
                              
     About the link: This entry is from Wolfram, which is a software
    company known for Mathematica.  
        
     Terms of Use: Please respect the copyright and terms of use
    displayed on the web pages above.

**4.2.2 Feedforward Neural Networks** <span id="4.2.2"></span> 
-   **Reading: Wolfram Mathematica’s “Feedforward Neural Networks”**
    Link: Wolfram Mathematica’s “[Feedforward Neural
    Networks](http://reference.wolfram.com/applications/neuralnetworks/NeuralNetworkTheory/2.5.1.html)”
    (HTML)  
        
     Instructions: Read only the pages under section 2.5., which covers
    theFeedforward neural network.  Make sure you understand this
    network’s mathematical definition and that you study the examples in
    figures 2.5 and 2.6.  
        
     Terms of Use: Please respect the copyright and terms of use
    displayed on the web pages above.

**4.2.3 Radial Basis Function Networks** <span id="4.2.3"></span> 
-   **Reading: Wolfram Mathematica’s “Radial Basis Function Networks”**
    Link: Wolfram Mathematica’s “[Radial Basis Function
    Networks”](http://reference.wolfram.com/applications/neuralnetworks/NeuralNetworkTheory/2.5.2.html)
    (HTML)  
        
     Instructions: Read only the pages under the section 2.5.2, which
    coversthe Radial Basis Function Network.  Make sure youunderstand
    the network’s mathematical definition and be sure to study the
    examples in figures 2.7 and 2.8.  
        
     Terms of Use: Please respect the copyright and terms of use
    displayed on the web pages above.

**4.2.4 The Perceptron** <span id="4.2.4"></span> 
-   **Reading: Wolfram Mathematica’s “The Perceptron”**
    Link: Wolfram Mathematica’s “[The
    Perceptron](http://reference.wolfram.com/applications/neuralnetworks/NeuralNetworkTheory/2.4.0.html)”
    (HTML)  
        
     Instructions: Read only the pages under the section 2.4 about
    Perceptron.  Be sure to understand its mathematical definition,
    learn the training algorithm, and study the example in figure 2.4.  
        
     Terms of Use: Please respect the copyright and terms of use
    displayed on the web pages above.  
      

**4.2.5 Vector Quantization (VQ) Networks** <span id="4.2.5"></span> 
-   **Reading: Wolfram Mathematica’s “Vector Quantization Networks”**
    Link: Wolfram Mathematica’s “[Vector Quantization
    Networks](http://reference.wolfram.com/applications/neuralnetworks/NeuralNetworkTheory/2.8.0.html)”
    (HTML)  
        
     Instructions: Starting at the third to last paragraph in section
    2.8 which starts at “Another neural network type…”, read about the
    Vector Quantization network.  
        
     Terms of Use: Please respect the copyright and terms of use
    displayed on the web pages above.

**4.2.6 Hopfield Network** <span id="4.2.6"></span> 
-   **Reading: Wolfram Mathematica’s “Hopfield Network”**
    Link: Wolfram Mathematica’s “[Hopfield
    Network](http://reference.wolfram.com/applications/neuralnetworks/NeuralNetworkTheory/2.7.0.html)”
    (HTML)  
        
     Instructions: Read only the pages under the section 2.7, which
    presents the Hopfield network and the equations that describe it.  
        
     Terms of Use: Please respect the copyright and terms of use
    displayed on the web pages above.

**4.3 Other Classifiers and Statistical Learning Methods** <span
id="4.3"></span> 
**4.3.1 Kernel Methods** <span id="4.3.1"></span> 
-   **Reading: Wikipedia’s “Kernel Methods”**
    Link: Wikipedia’s “[Kernel
    Methods](http://www.saylor.org/site/wp-content/uploads/2012/06/Wikipedia-Kernel-Methods.pdf)”
    (PDF)  
        
     Instructions: Read this web page to review Kernel methods.  Focus
    on the introductory part at the top of the text, i.e. the basic
    description and definition.  
        
     Terms of Use: The article above is released under a [Creative
    Commons Attribution-Share Alike License
    3.0](http://creativecommons.org/licenses/by-sa/3.0/) (HTML).  You
    can find the Wikipedia source article
    [here](http://en.wikipedia.org/wiki/Kernel_methods) (HTML).

**4.3.2 k-nearest Neighbor Algorithm** <span id="4.3.2"></span> 
-   **Reading: Wikipedia’s “k-nearest Neighbor Algorithm”**
    Link: Wikipedia’s “[k-nearest Neighbor
    Algorithm](http://www.saylor.org/site/wp-content/uploads/2012/06/Wikipedia-k-Nearest-Neighbor-Algorithm.pdf)”
    (PDF)  
        
     Instructions: Make sure you know how the k-nearest neighbor
    algorithm works (in principle) after reading this entry.  
        
     Terms of Use: The article above is released under a [Creative
    Commons Attribution-Share Alike License
    3.0](http://creativecommons.org/licenses/by-sa/3.0/) (HTML).  You
    can find the Wikipedia source article
    [here](http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm)
    (HTML).

**4.3.3 Mixture Model** <span id="4.3.3"></span> 
-   **Reading: Wikipedia’s “Mixture Model”**
    Link: Wikipedia’s “[Mixture
    Model](http://www.saylor.org/site/wp-content/uploads/2012/06/Wikipedia-Mixture-Model.pdf)”
    (PDF)  
        
     Instructions: Read this web page to learn about the different types
    of Mixture Models.  Pay attention to the 'General Mixture Model'
    section and read as much as you can from the 'Specific examples' and
    'Examples' sections.  
        
     Terms of Use: The article above is released under a [Creative
    Commons Attribution-Share Alike License
    3.0](http://creativecommons.org/licenses/by-sa/3.0/) (HTML).  You
    can find the Wikipedia source article
    [here](http://en.wikipedia.org/wiki/Mixture_model) (HTML).

**4.3.4 Naive Bayes Classifier** <span id="4.3.4"></span> 
-   **Reading: Wikipedia’s “Naive Bayes Classifier”**
    Link: Wikipedia’s “[Naive Bayes
    Classifier](http://www.saylor.org/site/wp-content/uploads/2012/06/Wikipedia-Naive-Bayes-Classifier.pdf)”
    (PDF)  
        
     Instructions: After reading the linked material, make sure you know
    the definition of the naive Bayes classifier.  Work through the 'The
    Naive Bayes Probabilistic Model' and 'Examples' sections.       
        
     Terms of Use: The article above is released under a [Creative
    Commons Attribution-Share Alike License
    3.0](http://creativecommons.org/licenses/by-sa/3.0/) (HTML).  You
    can find the Wikipedia source article
    [here](http://en.wikipedia.org/wiki/Naive_Bayes_classifier) (HTML).

**4.3.5 Decision Tree** <span id="4.3.5"></span> 
-   **Reading: Wikipedia’s “Decision Tree”**
    Link: Wikipedia’s “[Decision
    Tree](http://www.saylor.org/site/wp-content/uploads/2012/06/Wikipedia-Decision-Tree.pdf)”
    (PDF)  
        
     Instructions: Read this entry; you should be able to define the
    term “decision tree” when you are done.  
        
     Terms of Use: The article above is released under a [Creative
    Commons Attribution-Share Alike License
    3.0](http://creativecommons.org/licenses/by-sa/3.0/) (HTML).  You
    can find the Wikipedia source article
    [here](http://en.wikipedia.org/wiki/Decision_tree) (HTML).

**4.3.6 Kernels and Gaussian Processes** <span id="4.3.6"></span> 
-   **Lecture: videolectures.net’s: Mark Girolami’s “Kernels and
    Gaussian Processes: Parts 1-3”**
    Link: videolectures.net’s: Mark Girolami’s “[Kernels and Gaussian
    Processes: Parts
    1-3](http://videolectures.net/bootcamp07_girolami_kgp/)” (Adobe
    Flash and Windows Media Player)  
                              
     Instructions: Watch the first video about machine learningand
    compare it to what you have learned in the readings from the
    sections above.  After watching this video, you should the basics of
    linear regression, loss function, prediction techniques. Study
    non-linear models, probabilistic regression, and uncertainty
    estimation.  This lecture is just over 1 hour long.  You may wish to
    work through the slides provided on the right-hand side of the
    screen as you work through this lecture and the two below. You can
    also download the PowerPoint slides in a PDF format by clicking on
    the links under “See Also.”  
        
     Then, watch the second video lecture to learn about Bayesian
    regression and classification.  This second lecture is 1 hour
    long.  
        
     Finally, watch the last lecture to learn about Gaussian processes,
    regression, and classification.  This third installment is just over
    1 hour long.  
        
     About the link: Mark Girolami is a professor at the University of
    Glasgow.  
        
     Terms of Use: The article above is released under a [Creative
    Commons Attribution-NonCommercial- NoDerivatives License
    3.0](http://creativecommons.org/licenses/by-nc-nd/3.0/)(HTML).  It
    is attributed to (Mark Girolami).

**4.4 Machine Learning Coding Drills** <span id="4.4"></span> 
-   **Assignment: Artificial Intelligence Center’s “Tic-Tac-Toe”**
    Link: Artificial Intelligence Center’s [“Tic-Tac-Toe
    Demo”](https://code.google.com/p/java-channel-tic-tac-toe/source/checkout)
    (JAVA)  
        
     Instructions: Code an agent that plays the Tic-Tac-Toe game.  You
    can choose to play the game yourself by selecting board positions or
    have the Agent propose moves.  One possible solution is available
    via the link above under the Tic-Tac-Toe Demo section.  Work towards
    a solution for no more than 10 hours and then check your work
    against the solution code.  
        
     About the link: The code above is a Java implementation of
    algorithms from Norvig And Russell's "Artificial Intelligence - A
    Modern Approach,” 3rd Edition.  
        
     Terms of Use: Please respect the copyright and terms of use
    displayed on the web page above.


